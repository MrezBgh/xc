{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file shows the gradual change in predominance during the training sessions of Dieter task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from scipy.stats import norm, stats\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "from collections import defaultdict\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Plotting Functions\n",
    "''''''\n",
    "\n",
    "def plot_fish(fish_state, ax=None, show=True):\n",
    "  \"\"\"\n",
    "  Plot the fish dynamics (states across time)\n",
    "  \"\"\"\n",
    "  T = len(fish_state)\n",
    "\n",
    "  offset = 3\n",
    "\n",
    "  if not ax:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 3.5))\n",
    "\n",
    "  x = np.arange(0, T, 1)\n",
    "  y = offset * (fish_state*2 - 1)\n",
    "\n",
    "  ax.plot(y, color='cornflowerblue', markersize=10, linewidth=3.0, zorder=0)\n",
    "  ax.fill_between(x, y, color='cornflowerblue', alpha=.3)\n",
    "\n",
    "  ax.set_xlabel('time')\n",
    "  ax.set_ylabel('Chosen location')\n",
    "\n",
    "  ax.set_xlim([0, T])\n",
    "  ax.set_xticks([])\n",
    "  ax.xaxis.set_label_coords(1.05, .54)\n",
    "\n",
    "  ax.set_ylim([-(offset+.5), offset+.5])\n",
    "  ax.set_yticks([-offset, offset])\n",
    "  ax.set_yticklabels(['left', 'right'])\n",
    "\n",
    "  ax.spines['bottom'].set_position('center')\n",
    "  if show:\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_measurement(measurement, ax=None, show=True):\n",
    "  \"\"\"\n",
    "  Plot the measurements\n",
    "  \"\"\"\n",
    "  T = len(measurement)\n",
    "\n",
    "  rel_pos = 3\n",
    "  red_y = []\n",
    "  blue_y = []\n",
    "  for idx, value in enumerate(measurement):\n",
    "    if value == 0:\n",
    "      blue_y.append([idx, -rel_pos])\n",
    "    else:\n",
    "      red_y.append([idx, rel_pos])\n",
    "\n",
    "  red_y = np.asarray(red_y)\n",
    "  blue_y = np.asarray(blue_y)\n",
    "\n",
    "  if not ax:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 3.5))\n",
    "\n",
    "  if len(red_y) > 0:\n",
    "    ax.plot(red_y[:, 0], red_y[:, 1], '*', markersize=8, color='crimson')\n",
    "\n",
    "  if len(blue_y) > 0:\n",
    "    ax.plot(blue_y[:, 0], blue_y[:, 1], '*', markersize=8, color='royalblue')\n",
    "\n",
    "  ax.set_xlabel('time', fontsize=18)\n",
    "  ax.set_ylabel('Caught fish?')\n",
    "\n",
    "  ax.set_xlim([0, T])\n",
    "  ax.set_xticks([])\n",
    "  ax.xaxis.set_label_coords(1.05, .54)\n",
    "\n",
    "  ax.set_ylim([-rel_pos - .5, rel_pos + .5])\n",
    "  ax.set_yticks([-rel_pos, rel_pos])\n",
    "  ax.set_yticklabels(['no', 'yes!'])\n",
    "\n",
    "  ax.spines['bottom'].set_position('center')\n",
    "  if show:\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_act_loc(loc, act, ax_loc=None, show=True):\n",
    "  \"\"\"\n",
    "  Plot the action and location of T time points\n",
    "  \"\"\"\n",
    "  T = len(act)\n",
    "\n",
    "  if not ax_loc:\n",
    "    fig, ax_loc = plt.subplots(1, 1, figsize=(12, 2.5))\n",
    "\n",
    "  loc = loc*2 - 1\n",
    "  act_down = []\n",
    "  act_up = []\n",
    "  for t in range(1, T):\n",
    "    if loc[t-1] == -1 and loc[t] == 1:\n",
    "      act_up.append([t - 0.5, 0])\n",
    "    if loc[t-1] == 1 and loc[t] == -1:\n",
    "      act_down.append([t - 0.5, 0])\n",
    "\n",
    "  act_down = np.array(act_down)\n",
    "  act_up = np.array(act_up)\n",
    "\n",
    "  ax_loc.plot(loc, 'g.-', markersize=8, linewidth=5)\n",
    "\n",
    "  if len(act_down) > 0:\n",
    "    ax_loc.plot(act_down[:, 0], act_down[:, 1], 'rv', markersize=18, zorder=10, label='switch')\n",
    "\n",
    "  if len(act_up) > 0:\n",
    "    ax_loc.plot(act_up[:, 0], act_up[:, 1], 'r^', markersize=18, zorder=10)\n",
    "\n",
    "  ax_loc.set_xlabel('time')\n",
    "  ax_loc.set_ylabel('Your state')\n",
    "\n",
    "  ax_loc.set_xlim([0, T])\n",
    "  ax_loc.set_xticks([])\n",
    "  ax_loc.xaxis.set_label_coords(1.05, .54)\n",
    "\n",
    "  if len(act_down) > 0:\n",
    "    ax_loc.legend(loc=\"upper right\")\n",
    "  elif len(act_down) == 0 and len(act_up) > 0:\n",
    "    ax_loc.plot(act_up[:, 0], act_up[:, 1], 'r^', markersize=18, zorder=10, label='switch')\n",
    "    ax_loc.legend(loc=\"upper right\")\n",
    "\n",
    "  ax_loc.set_ylim([-1.1, 1.1])\n",
    "  ax_loc.set_yticks([-1, 1])\n",
    "\n",
    "  ax_loc.tick_params(axis='both', which='major')\n",
    "  ax_loc.set_yticklabels(['left', 'right'])\n",
    "\n",
    "  ax_loc.spines['bottom'].set_position('center')\n",
    "\n",
    "  if show:\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_belief(belief, ax1=None, choose_policy=None, show=True):\n",
    "  \"\"\"\n",
    "  Plot the belief dynamics of T time points\n",
    "  \"\"\"\n",
    "\n",
    "  T = belief.shape[1]\n",
    "\n",
    "  if not ax1:\n",
    "      fig, ax1 = plt.subplots(1, 1, figsize=(12, 2.5))\n",
    "\n",
    "  ax1.plot(belief[1, :], color='midnightblue', markersize=10, linewidth=3.0)\n",
    "\n",
    "  ax1.set_xlabel('time')\n",
    "  ax1.set_ylabel('Belief (right)')\n",
    "\n",
    "  ax1.set_xlim([0, T])\n",
    "  ax1.set_xticks([])\n",
    "  ax1.xaxis.set_label_coords(1.05, 0.05)\n",
    "\n",
    "  ax1.set_yticks([0, 1])\n",
    "  ax1.set_ylim([0, 1.1])\n",
    "\n",
    "  labels = [item.get_text() for item in ax1.get_yticklabels()]\n",
    "  ax1.set_yticklabels(['    0', '    1'])\n",
    "\n",
    "  \"\"\"\n",
    "  if choose_policy == \"threshold\":\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(time_range, threshold * np.ones(time_range.shape), 'r--')\n",
    "    ax2.plot(time_range, (1 - threshold) * np.ones(time_range.shape), 'c--')\n",
    "    ax2.set_yticks([threshold, 1 - threshold])\n",
    "    ax2.set_ylim([0, 1.1])\n",
    "    ax2.tick_params(axis='both', which='major', labelsize=18)\n",
    "    labels = [item.get_text() for item in ax2.get_yticklabels()]\n",
    "    labels[0] = 'threshold to switch \\n from left to right'\n",
    "    labels[-1] = 'threshold to switch \\n from right to left'\n",
    "    ax2.set_yticklabels(labels)\n",
    "  \"\"\"\n",
    "  if show:\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_dynamics(belief, loc, act, meas, fish_state, choose_policy):\n",
    "  \"\"\"\n",
    "  Plot the dynamics of T time points\n",
    "  \"\"\"\n",
    "  if choose_policy == 'threshold':\n",
    "    fig, [ax0, ax_bel, ax_loc, ax1] = plt.subplots(4, 1, figsize=(12, 9))\n",
    "    plot_fish(fish_state, ax=ax0, show=False)\n",
    "    plot_belief(belief, ax1=ax_bel, show=False)\n",
    "    plot_measurement(meas, ax=ax1, show=False)\n",
    "    plot_act_loc(loc, act, ax_loc=ax_loc)\n",
    "  else:\n",
    "    fig, [ax0, ax_bel, ax1] = plt.subplots(3, 1, figsize=(12, 7))\n",
    "    plot_fish(fish_state, ax=ax0, show=False)\n",
    "    plot_belief(belief, ax1=ax_bel, show=False)\n",
    "    plot_measurement(meas, ax=ax1, show=False)\n",
    "\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def belief_histogram(belief, bins=100):\n",
    "  \"\"\"\n",
    "  Plot the histogram of belief states\n",
    "  \"\"\"\n",
    "  fig, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "  ax.hist(belief, bins)\n",
    "  ax.set_xlabel('belief', fontsize=18)\n",
    "  ax.set_ylabel('count', fontsize=18)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def plot_value_threshold(threshold_array, value_array):\n",
    "  \"\"\"\n",
    "  Helper function to plot the value function and threshold\n",
    "  \"\"\"\n",
    "  yrange = np.max(value_array) - np.min(value_array)\n",
    "  star_loc = np.argmax(value_array)\n",
    "\n",
    "  fig_, ax = plt.subplots(1, 1, figsize=(8, 6))\n",
    "  ax.plot(threshold_array, value_array, 'b')\n",
    "  ax.vlines(threshold_array[star_loc],\n",
    "            min(value_array) - yrange * .1, max(value_array),\n",
    "            colors='red', ls='--')\n",
    "  ax.plot(threshold_array[star_loc],\n",
    "          value_array[star_loc],\n",
    "          '*', color='crimson',\n",
    "          markersize=20)\n",
    "\n",
    "  ax.set_ylim([np.min(value_array) - yrange * .1,\n",
    "               np.max(value_array) + yrange * .1])\n",
    "  ax.set_title(f'threshold vs value with switching cost c = {cost_sw:.2f}',\n",
    "               fontsize=20)\n",
    "  ax.set_xlabel('threshold', fontsize=16)\n",
    "  ax.set_ylabel('value', fontsize=16)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "\n",
    "    \"\"\"A Markov Decision Process, defined by an initial state, transition model,\n",
    "    and reward function. We also keep track of a gamma value, for use by\n",
    "    algorithms. The transition model is represented somewhat differently from\n",
    "    the text. Instead of P(s' | s, a) being a probability number for each\n",
    "    state/state/action triplet, we instead have T(s, a) return a\n",
    "    list of (p, s') pairs. We also keep track of the possible states,\n",
    "    terminal states, and actions for each state. [page 646]\"\"\"\n",
    "\n",
    "    def __init__(self, init, actlist, terminals, transitions = {}, reward = None, states=None, gamma=.9):\n",
    "        if not (0 < gamma <= 1):\n",
    "            raise ValueError(\"An MDP must have 0 < gamma <= 1\")\n",
    "\n",
    "        if states:\n",
    "            self.states = states\n",
    "        else:\n",
    "            ## collect states from transitions table\n",
    "            self.states = self.get_states_from_transitions(transitions)\n",
    "            \n",
    "        \n",
    "        self.init = init\n",
    "        \n",
    "        if isinstance(actlist, list):\n",
    "            ## if actlist is a list, all states have the same actions\n",
    "            self.actlist = actlist\n",
    "        elif isinstance(actlist, dict):\n",
    "            ## if actlist is a dict, different actions for each state\n",
    "            self.actlist = actlist\n",
    "        \n",
    "        self.terminals = terminals\n",
    "        self.transitions = transitions\n",
    "        if self.transitions == {}:\n",
    "            print(\"Warning: Transition table is empty.\")\n",
    "        self.gamma = gamma\n",
    "        if reward:\n",
    "            self.reward = reward\n",
    "        else:\n",
    "            self.reward = {s : 0 for s in self.states}\n",
    "        #self.check_consistency()\n",
    "\n",
    "    def R(self, state):\n",
    "        \"\"\"Return a numeric reward for this state.\"\"\"\n",
    "        return self.reward[state]\n",
    "\n",
    "    def T(self, state, action):\n",
    "        \"\"\"Transition model. From a state and an action, return a list\n",
    "        of (probability, result-state) pairs.\"\"\"\n",
    "        if(self.transitions == {}):\n",
    "            raise ValueError(\"Transition model is missing\")\n",
    "        else:\n",
    "            return self.transitions[state][action]\n",
    "\n",
    "    def actions(self, state):\n",
    "        \"\"\"Set of actions that can be performed in this state. By default, a\n",
    "        fixed list of actions, except for terminal states. Override this\n",
    "        method if you need to specialize by state.\"\"\"\n",
    "        if state in self.terminals:\n",
    "            return [None]\n",
    "        else:\n",
    "            return self.actlist\n",
    "\n",
    "    def get_states_from_transitions(self, transitions):\n",
    "        if isinstance(transitions, dict):\n",
    "            s1 = set(transitions.keys())\n",
    "            s2 = set([tr[1] for actions in transitions.values() \n",
    "                              for effects in actions.values() for tr in effects])\n",
    "            return s1.union(s2)\n",
    "        else:\n",
    "            print('Could not retrieve states from transitions')\n",
    "            return None\n",
    "\n",
    "    def check_consistency(self):\n",
    "        # check that all states in transitions are valid\n",
    "        assert set(self.states) == self.get_states_from_transitions(self.transitions)\n",
    "        # check that init is a valid state\n",
    "        assert self.init in self.states\n",
    "        # check reward for each state\n",
    "        #assert set(self.reward.keys()) == set(self.states)\n",
    "        assert set(self.reward.keys()) == set(self.states)\n",
    "        # check that all terminals are valid states\n",
    "        assert all([t in self.states for t in self.terminals])\n",
    "        # check that probability distributions for all actions sum to 1\n",
    "        for s1, actions in self.transitions.items():\n",
    "            for a in actions.keys():\n",
    "                s = 0\n",
    "                for o in actions[a]:\n",
    "                    s += o[0]\n",
    "                assert abs(s - 1) < 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POMDP(MDP):\n",
    "\n",
    "\n",
    "    def __init__(self, actions, transitions=None, rewards=None, states=None, p_stay = 0.95, sigmaAttention = np.array([.1, .02]), gamma=0.95, x = 5, y = 3, Initial_loc = 0):\n",
    "        \"\"\"Initialize variables of the pomdp\"\"\"\n",
    "        # States: 0: Pinwheel, 1: Bull's-eye horizontal, 2: Bull's-eye vertical\n",
    "        # Actions: 0: Pinwheel Perception, 1: Bull's-eye horizontal Perception, 2: Bull's-eye vertical Perception, 3: Key Bull's-eye horizontal, 4: Key Bull's-eye vertical\n",
    "        # x is number of actions, and y is number of states\n",
    "        if not (0 <= gamma <= 1):\n",
    "            raise ValueError('A POMDP must have 0 < gamma <= 1')\n",
    "        #p_stay and sigma attention are related to updating the belief \n",
    "        self.p_stay = p_stay\n",
    "        self.sigmaAttention = sigmaAttention\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.rewards = rewards\n",
    "        # self.t_prob = np.array([[self.p_stay, (1-self.p_stay)/2, (1-self.p_stay)/2],\n",
    "        #                          [(1-self.p_stay)/2, self.p_stay, (1-self.p_stay)/2],\n",
    "        #                            [(1-self.p_stay)/2, (1-self.p_stay)/2, self.p_stay]])\n",
    "        self.t_prob = transitions\n",
    "        # transition model cannot be undefined\n",
    "        # transitions = p_stay\n",
    "        \n",
    "        #t_prob is (2,2,2) 3d matrix. First dimension determines the action choice. Second dimension determines the state which the agent was in.\n",
    "        # if not isinstance(transitions, (np.ndarray, list)):\n",
    "        #     self.t_prob = np.array([[[transitions, 1-transitions] for _ in range(x)] for _ in range(y)], dtype=float)\n",
    "        # else:\n",
    "        #     self.t_prob = transitions\n",
    "\n",
    "        # \"\"\"if not self.t_prob:\n",
    "        # print('Warning: Transition model is undefined')\"\"\"\n",
    "\n",
    "        '''# sensor model cannot be undefined\n",
    "        # self.e_prob = evidences or {}\n",
    "        # if not self.e_prob:\n",
    "        #     print('Warning: Sensor model is undefined')'''\n",
    "\n",
    "        self.gamma = gamma\n",
    "    \n",
    "        #prev_state determines the previous action\n",
    "        self.prev_state = Initial_loc\n",
    "        # history is a number that determines the number of same actions which are chosen consequently\n",
    "        self.history = 1\n",
    "        # These lists are defined to save previous variants\n",
    "        self.actlist = []\n",
    "        self.statelist = []\n",
    "        self.belieflist = []\n",
    "        self.rewardlist = [[] for i in range (y)]\n",
    "        # Initial reward (It is randomy assigned)\n",
    "        # The dimensions of the reward array determine from which state the agent goes to which state\n",
    "        self.reward_arr = np.full((x,y),1,dtype=float)\n",
    "        self.fu_reward_arr = np.full((x,y),2,dtype=float)\n",
    "        self.max_fu_reward = np.full((y),2,dtype=float)\n",
    "        \n",
    "        self.e_prob = np.random.dirichlet(np.ones(y))\n",
    "        self.belief = np.random.dirichlet(np.ones(y))\n",
    "        # print(self.t_prob)\n",
    "    # chooses the actions based on the maximum utility of the previous state \n",
    "    def selected_action(self, utility : list):\n",
    "        act = np.argmax(utility)\n",
    "        return act\n",
    "    \n",
    "    def outcome(self, utility):\n",
    "        # outcome_0 = np.sum(utility[0]*self.t_prob[0][int(self.prev_state)])\n",
    "        # outcome_1 = np.sum(utility[1]*self.t_prob[1][int(self.prev_state)])\n",
    "        outcome = np.zeros(len(self.actions), dtype= float)\n",
    "        for i in range(len(utility)):\n",
    "            outcome[i] = np.sum(utility[i])\n",
    "        return outcome\n",
    "    \n",
    "    # Based on the chosen action and transition probabilities, it determines which state would be the next one which the agent will be placed\n",
    "    def last_state(self, action):\n",
    "        internal_action = [0, 1, 2]\n",
    "        external_action = [3, 4]\n",
    "        if action in internal_action:\n",
    "            state = str(action)\n",
    "        else:\n",
    "            state = self.prev_state \n",
    "        return state\n",
    "    \n",
    "    # updates the reward, it generates a (2,2) array (more explanation in reward_arr initialization)\n",
    "    def reward_set (self):\n",
    "\n",
    "        \"\"\"\n",
    "            rewards = (rewDom, c1, c2, alpha , expts, key_rew, fixrewardstep)\n",
    "            rewards = (1,                   1,                  10,     2,      5)\"\"\"\n",
    "        rewDom, c1, c2, alpha , expts, key_rew, punish_rew_1, punish_rew_2, consfixed, biasL, biasR = self.rewards\n",
    "        exp_decay = rewDom * np.exp(-self.history/expts)\n",
    "        rewsupp = rewDom * np.exp(-consfixed/expts)\n",
    "\n",
    "\n",
    "        if self.prev_state == '0':\n",
    "\n",
    "            self.reward_arr[0,0] = exp_decay*biasL\n",
    "            self.reward_arr[1,1] = rewsupp\n",
    "            self.reward_arr[2,2] = rewsupp\n",
    "            self.reward_arr[0,1] = self.reward_arr[0,2] = self.reward_arr[1,0] = self.reward_arr[2,0] = c2\n",
    "            self.reward_arr[1,2] = self.reward_arr[2,1] = c1\n",
    "            self.reward_arr[3,1] = self.reward_arr[4,2] = key_rew\n",
    "            self.reward_arr[3,2] = self.reward_arr[4,1] = -punish_rew_1\n",
    "            self.reward_arr[3,0] = self.reward_arr[4,0] = -punish_rew_2 \n",
    "\n",
    "        elif self.prev_state == '1':\n",
    "\n",
    "            self.reward_arr[0,0] = rewsupp\n",
    "            self.reward_arr[1,1] = exp_decay*biasR\n",
    "            self.reward_arr[2,2] = exp_decay * alpha*biasR\n",
    "            self.reward_arr[0,1] = self.reward_arr[0,2] = self.reward_arr[1,0] = self.reward_arr[2,0] = c2\n",
    "            self.reward_arr[1,2] = self.reward_arr[2,1] = c1\n",
    "            self.reward_arr[3,1] = self.reward_arr[4,2] = key_rew\n",
    "            self.reward_arr[3,2] = self.reward_arr[4,1] = -punish_rew_1\n",
    "            self.reward_arr[3,0] = self.reward_arr[4,0] = -punish_rew_2 \n",
    "\n",
    "        elif self.prev_state == '2':\n",
    "\n",
    "            self.reward_arr[0,0] = rewsupp\n",
    "            self.reward_arr[1,1] = exp_decay * alpha*biasR\n",
    "            self.reward_arr[2,2] = exp_decay * biasR\n",
    "            self.reward_arr[0,1] = self.reward_arr[0,2] = self.reward_arr[1,0] = self.reward_arr[2,0] = c2\n",
    "            self.reward_arr[1,2] = self.reward_arr[2,1] = c1\n",
    "            self.reward_arr[3,1] = self.reward_arr[4,2] = key_rew\n",
    "            self.reward_arr[3,2] = self.reward_arr[4,1] = -punish_rew_1\n",
    "            self.reward_arr[3,0] = self.reward_arr[4,0] = -punish_rew_2 \n",
    "\n",
    "    def belief_update(self):\n",
    "        # opp_state = np.abs(1-prev_state)\n",
    "        # state = int(self.prev_state)\n",
    "\n",
    "        # evaluatuing belief_0\n",
    "        # print('pp', type(self.prev_state)).\n",
    "        # print(self.sigmaAttention)\n",
    "        if self.prev_state == '0':\n",
    "\n",
    "            sigma1 = self.sigmaAttention[0] \n",
    "            sigma2 = self.sigmaAttention[1]\n",
    "            sigma3 = self.sigmaAttention[1]  \n",
    "\n",
    "        elif self.prev_state == '1': \n",
    "\n",
    "            sigma1 = self.sigmaAttention[1] \n",
    "            sigma2 = self.sigmaAttention[0]\n",
    "            sigma3 = self.sigmaAttention[2]\n",
    "\n",
    "        elif self.prev_state == '2':\n",
    "\n",
    "            sigma1 = self.sigmaAttention[1] \n",
    "            sigma2 = self.sigmaAttention[2]\n",
    "            sigma3 = self.sigmaAttention[0]    \n",
    "        \n",
    "        mu = 0\n",
    "        obsVal = np.random.normal(mu, sigma1, 1)\n",
    "        obsProb0 = norm.pdf(obsVal, loc=mu, scale=sigma1)\n",
    "\n",
    "        mu = 1\n",
    "        obsVal = np.random.normal(mu, sigma2, 1)\n",
    "        obsProb1 = norm.pdf(obsVal, loc=mu, scale=sigma2)\n",
    "\n",
    "        mu = 2\n",
    "        obsVal = np.random.normal(mu, sigma3, 1)\n",
    "        obsProb2 = norm.pdf(obsVal, loc=mu, scale=sigma3)\n",
    "\n",
    "        observation_array = np.concatenate((obsProb0, obsProb1, obsProb2))\n",
    "        # print('observation_array', observation_array)\n",
    "        belief_arr = np.multiply(np.matmul(np.transpose(self.t_prob), np.transpose(self.belief)), observation_array)\n",
    "\n",
    "        self.belief = belief_arr/np.sum(belief_arr)\n",
    "        self.belieflist.append(belief_arr[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    #at the end of each point, updates the variants\n",
    "    def update(self, st, action):\n",
    "        # for i in range(len(self.rewardlist)):\n",
    "        #     self.rewardlist[i].append(self.reward_arr[self.prev_state,i])\n",
    "        if st != '0':\n",
    "            if self.prev_state != '0':\n",
    "                self.history += 1\n",
    "            else:\n",
    "                self.history = 0\n",
    "        else:\n",
    "            if self.prev_state == '0':\n",
    "                self.history += 1\n",
    "            else:\n",
    "                self.history = 0\n",
    "                \n",
    "        self.prev_state = st\n",
    "        self.actlist.append(action)\n",
    "        self.statelist.append(st)\n",
    "\n",
    "\n",
    "    def reward_generation(self,hist,prev_state):\n",
    "                \n",
    "        \"\"\"reward[0, t] = biasRewFactorLeft * rewDom * np.exp(-t_inSw/expts)\n",
    "        rewards = (biasRewFactorLeft, biasRewFactorRight, rewDom, rewSupp, expts)\n",
    "        rewards = (1,                   1,                  10,     2,      5)\"\"\"\n",
    "\n",
    "        biasRewFactorLeft, biasRewFactorRight, rewDom, rewSupp, expts = self.rewards\n",
    "        biasRewFactor = {'0' : biasRewFactorLeft, '1' : biasRewFactorRight}\n",
    "\n",
    "\n",
    "        if prev_state == '0':\n",
    "            # self.reward_arr[0,1] = self.reward_arr[0,1] \n",
    "            # self.reward_arr[1,0] = self.reward_arr[1,0] \n",
    "            # self.reward_arr[0,1] = rewSupp\n",
    "            # self.reward_arr[1,1] = rewSupp\n",
    "            # self.reward_arr[0,0] = biasRewFactor[\"0\"] * rewDom * np.exp(-self.history/expts)\n",
    "            # self.reward_arr[1,0] = biasRewFactor[\"0\"] * rewDom * np.exp(-self.history/expts)\n",
    "            self.fu_reward_arr[0,1] = rewSupp\n",
    "            self.fu_reward_arr[0,0] = biasRewFactor[\"0\"] * rewDom * np.exp(-hist/expts)\n",
    "            self.fu_reward_arr[1,0] = rewSupp\n",
    "            self.fu_reward_arr[1,1] = biasRewFactor[\"1\"] * rewDom * np.exp(-1/expts)\n",
    "        \n",
    "\n",
    "        elif prev_state == '1':\n",
    "            # self.reward_arr[0,1] = self.reward_arr[1,1] \n",
    "            # self.reward_arr[1,0] = self.reward_arr[0,0] \n",
    "            # self.reward_arr[0,0] = rewSupp\n",
    "            # self.reward_arr[1,0] = rewSupp\n",
    "            # self.reward_arr[0,1] = biasRewFactor[\"1\"] * rewDom * np.exp(-self.history/expts)\n",
    "            # self.reward_arr[1,1] = biasRewFactor[\"1\"] * rewDom * np.exp(-self.history/expts)\n",
    "            self.fu_reward_arr[1,0] = rewSupp\n",
    "            self.fu_reward_arr[1,1] = biasRewFactor[\"1\"] * rewDom * np.exp(-hist/expts)\n",
    "            self.fu_reward_arr[0,1] = rewSupp\n",
    "            self.fu_reward_arr[0,0] = biasRewFactor[\"0\"] * rewDom * np.exp(-1/expts)\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "    def remove_dominated_plans(self, input_values):\n",
    "        \"\"\"\n",
    "        Remove dominated plans.\n",
    "        This method finds all the lines contributing to the\n",
    "        upper surface and removes those which don't.\n",
    "        \"\"\"\n",
    "\n",
    "        values = [val for action in input_values for val in input_values[action]]\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        best = [values[0]]\n",
    "        y1_max = max(val[1] for val in values)\n",
    "        tgt = values[0]\n",
    "        prev_b = 0\n",
    "        prev_ix = 0\n",
    "        while tgt[1] != y1_max:\n",
    "            min_b = 1\n",
    "            min_ix = 0\n",
    "            for i in range(prev_ix + 1, len(values)):\n",
    "                if values[i][0] - tgt[0] + tgt[1] - values[i][1] != 0:\n",
    "                    trans_b = (values[i][0] - tgt[0]) / (values[i][0] - tgt[0] + tgt[1] - values[i][1])\n",
    "                    if 0 <= trans_b <= 1 and trans_b > prev_b and trans_b < min_b:\n",
    "                        min_b = trans_b\n",
    "                        min_ix = i\n",
    "            prev_b = min_b\n",
    "            prev_ix = min_ix\n",
    "            tgt = values[min_ix]\n",
    "            best.append(tgt)\n",
    "\n",
    "        return self.generate_mapping(best, input_values)\n",
    "\n",
    "    def remove_dominated_plans_fast(self, input_values):\n",
    "        \"\"\"\n",
    "        Remove dominated plans using approximations.\n",
    "        Resamples the upper boundary at intervals of 100 and\n",
    "        finds the maximum values at these points.\n",
    "        \"\"\"\n",
    "\n",
    "        values = [val for action in input_values for val in input_values[action]]\n",
    "        values.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "        best = []\n",
    "        sr = 100\n",
    "        for i in range(sr + 1):\n",
    "            x = i / float(sr)\n",
    "            maximum = (values[0][1] - values[0][0]) * x + values[0][0]\n",
    "            tgt = values[0]\n",
    "            for value in values:\n",
    "                val = (value[1] - value[0]) * x + value[0]\n",
    "                if val > maximum:\n",
    "                    maximum = val\n",
    "                    tgt = value\n",
    "            \n",
    "            if all(any(tgt != v) for v in best):\n",
    "                best.append(np.array(tgt))\n",
    "\n",
    "        return self.generate_mapping(best, input_values)\n",
    "\n",
    "    def generate_mapping(self, best, input_values):\n",
    "        \"\"\"Generate mappings after removing dominated plans\"\"\"\n",
    "\n",
    "        mapping = defaultdict(list)\n",
    "        # print('best: ',best)\n",
    "        # print('input: ',input_values)\n",
    "        for value in best:\n",
    "            for action in input_values:\n",
    "                if any(all(value == v) for v in input_values[action]):\n",
    "                    mapping[action].append(value)\n",
    "        # print(mapping)\n",
    "        return mapping\n",
    "\n",
    "    def max_difference(self, U1, U2):\n",
    "        \"\"\"Find maximum difference between two utility mappings\"\"\"\n",
    "\n",
    "        for k, v in U1.items():\n",
    "            sum1 = 0\n",
    "            for element in U1[k]:\n",
    "                sum1 += sum(element)\n",
    "            sum2 = 0\n",
    "            for element in U2[k]:\n",
    "                sum2 += sum(element)\n",
    "        return abs(sum1 - sum2)\n",
    "    \n",
    "\n",
    "    def belief_generation(self):\n",
    "        \n",
    "        states = [i for i in self.states]\n",
    "        observation = [[] for i in self.states]\n",
    "\n",
    "        for i in states:\n",
    "            if i == '0':\n",
    "\n",
    "                sigma1 = self.sigmaAttention[0] \n",
    "                sigma2 = self.sigmaAttention[2]\n",
    "                sigma3 = self.sigmaAttention[2]  \n",
    "\n",
    "            elif i == '1': \n",
    "\n",
    "                sigma1 = self.sigmaAttention[1] \n",
    "                sigma2 = self.sigmaAttention[0]\n",
    "                sigma3 = self.sigmaAttention[2]\n",
    "\n",
    "            elif i == '2':\n",
    "\n",
    "                sigma1 = self.sigmaAttention[1] \n",
    "                sigma2 = self.sigmaAttention[2]\n",
    "                sigma3 = self.sigmaAttention[0]      \n",
    "            \n",
    "            mu = 0\n",
    "            obsVal = np.random.normal(mu, sigma1, 1)\n",
    "            obsProb0 = norm.pdf(obsVal, loc=mu, scale=sigma1)\n",
    "\n",
    "            mu = 1\n",
    "            obsVal = np.random.normal(mu, sigma2, 1)\n",
    "            obsProb1 = norm.pdf(obsVal, loc=mu, scale=sigma2)\n",
    "\n",
    "            mu = 2\n",
    "            obsVal = np.random.normal(mu, sigma3, 1)\n",
    "            obsProb2 = norm.pdf(obsVal, loc=mu, scale=sigma3)\n",
    "            observation_array = np.concatenate((obsProb0, obsProb1, obsProb2))\n",
    "\n",
    "            observation[int(i)] = observation_array/np.sum(observation_array)\n",
    "\n",
    "        self.e_prob = observation\n",
    "    \n",
    "    def future_reward(self, future_step, dic):\n",
    "                \n",
    "        ff = future_step\n",
    "\n",
    "        history = self.history\n",
    "        state = self.statelist[-1]\n",
    "\n",
    "        for x in range(1,ff):\n",
    "            seq_list = []\n",
    "            for i in range(2**x):\n",
    "\n",
    "                binary_number = f'{i:0{x}b}'  # Convert to binary and pad with leading zeros to ensure 10 digits\n",
    "                seq_list.append(binary_number)\n",
    "            # print(seq_list)\n",
    "            rew = None\n",
    "\n",
    "\n",
    "            for seq in seq_list:\n",
    "                hist = history\n",
    "                current_node = dic\n",
    "                prev_state = state\n",
    "                # print('hist: ', hist)\n",
    "                for j in range(len(seq)):\n",
    "                    current_node = current_node[seq[j]]\n",
    "                    if seq[j] == str(prev_state):\n",
    "                        hist += 1\n",
    "                        prev_state = seq[j]\n",
    "                    elif seq[j] != str(prev_state):\n",
    "                        hist = 0\n",
    "                        prev_state = seq[j]\n",
    "                # print('Seq: ', seq)\n",
    "                self.reward_generation(hist, int(prev_state))\n",
    "                # rew = copy.deepcopy(self.fu_reward_arr)\n",
    "                rew = copy.deepcopy(self.fu_reward_arr)\n",
    "                # rew = copy.deepcopy(self.reward_arr)\n",
    "                # print('Att reward: ', rew)\n",
    "                current_node['r'] = rew\n",
    "                # print('current_node[r]: ',current_node['r'])\n",
    "                # print('DDDIIIII: ', dic)\n",
    "        dic['r'] = copy.deepcopy(self.reward_arr)\n",
    "        return dic\n",
    "\n",
    "\n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_calculation(pomdp, future_step, epsilon=0.1):\n",
    "    \"\"\"Solving a POMDP by value iteration.\"\"\"\n",
    "    \n",
    "    U = {'':[[0]* len(pomdp.states)]}\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        prev_U = U\n",
    "        # print('iteratation: ')\n",
    "        values = [val for action in U for val in U[action]]\n",
    "        rewards = pomdp.reward_arr\n",
    "        pomdp.belief_generation()\n",
    "        # print('e_prob ', pomdp.e_prob)\n",
    "        # print('val is: ',values , '\\n')\n",
    "        value_matxs = []\n",
    "\n",
    "        for i in values:\n",
    "            for j in values:\n",
    "                for z in values:\n",
    "                    value_matxs.append([i, j, z])\n",
    "        # print('matrix is : ', value_matxs)\n",
    "        U1 = defaultdict(list)\n",
    "        # print('U1 defaults ', U1)\n",
    "        for u in value_matxs:\n",
    "            for action in pomdp.actions:\n",
    "                # print(u)\n",
    "                utility = np.matmul(np.multiply(pomdp.t_prob, np.multiply(pomdp.gamma, np.multiply(pomdp.e_prob, np.transpose(u)).T)),[[1],[1],[1]])\n",
    "                rew = np.matmul(np.multiply(np.tile([rewards[int(action)]], (3, 1)).T,pomdp.t_prob),[[1],[1],[1]])\n",
    "                u1 = np.add(utility, rew)\n",
    "                # print('u: ', u,'\\n')\n",
    "                # print('e: ', belief,'\\n')\n",
    "                # print('next_step', next_step_utility,'\\n')\n",
    "\n",
    "                # print('action: ', action)\n",
    "                \n",
    "                # print('rew', np.transpose([rew[int(action)]]))\n",
    "                # print('current:', current_utility)\n",
    "                # print('t_prob', pomdp.t_prob)\n",
    "                # u1 = np.matmul(pomdp.t_prob, current_utility)\n",
    "                # print('reward is: ', [rewards[int(action)]],'\\n')\n",
    "                # print('little u1 is: ',u1 ,'\\n')\n",
    "                u1 = np.transpose(u1)\n",
    "                U1[action].append(u1[0])\n",
    "                # print('U1 all possible: ', U1,'\\n')\n",
    "                \n",
    "\n",
    "\n",
    "        U = pomdp.remove_dominated_plans_fast(U1)\n",
    "        # print('U1', U1,'\\n')\n",
    "        \n",
    "        \n",
    "        # U = U1\n",
    "        # print('U final', U)\n",
    "        # print(2*'\\n')\n",
    "        # replace with U = pomdp.remove_dominated_plans(U1) for accurate calculations\n",
    "\n",
    "        if count > future_step:\n",
    "            \n",
    "            # if pomdp.max_difference(U, prev_U) < epsilon * (1 - pomdp.gamma) / pomdp.gamma:\n",
    "            for act in pomdp.actions:\n",
    "                if act not in U:\n",
    "                    max =np.sum(pomdp.belief * U1[act][0])\n",
    "                    arg = 0\n",
    "                    for j in range(1, len(U1[act])):\n",
    "                        # print(f'{j}: ', rewardfu[i][j])\n",
    "                        # print(f'{j}*e: ',np.sum(pomdp.e_prob * rewardfu[i][j]))\n",
    "                        now = np.sum(pomdp.belief * U1[act][j])\n",
    "                        if now > max:\n",
    "                            # print('Now')\n",
    "                            max = now\n",
    "                            arg = j\n",
    "                    U[act] = [U1[act][arg]]\n",
    "                #     U[i] = [np.array(rewards[int(i)])]\n",
    "            # print('UUUUUU: ',U)\n",
    "\n",
    "            U = [U[i] for i in pomdp.actions]\n",
    "            return U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pomdp_value_iteration(actions, rewards, transitions, states, p_stay, sigmaAttention, gamma, future_steps, iterations, switching_cost, Initial_loc,counter):\n",
    "\n",
    "\n",
    "    pomdp = POMDP(actions = actions, rewards = rewards, transitions=transitions, states = states, p_stay=p_stay, sigmaAttention = sigmaAttention, gamma = gamma, Initial_loc=Initial_loc)\n",
    "    utility = pomdp_value_iteration(pomdp, future_steps, iterations, switching_cost, counter = counter)\n",
    "    \n",
    "    return utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pomdp_value_iteration(pomdp, future_steps, iterations = 500, switching_cost = 2,  epsilon=0.1, Initial_loc=0,counter = 5000):\n",
    "    \"\"\"Solving a POMDP by value iteration.\"\"\"\n",
    "\n",
    "    pomdp.update(Initial_loc,Initial_loc)\n",
    "    # U = {'': [[0] * len(pomdp.states)]}\n",
    "    switching_cost = switching_cost\n",
    "    count = 0\n",
    "    future_step = future_steps\n",
    "    # Variables below are used for plotting and storing times which agent spends between two switches\n",
    "    c0 = 0\n",
    "    c1 = 0\n",
    "\n",
    "    prev = Initial_loc\n",
    "    duration = [[] for i in range(2)]\n",
    "    all_states_duration = [[] for i in range(len(pomdp.states))]\n",
    "    ulist = [[] for i in range(len(pomdp.states))]\n",
    "    max_count = iterations\n",
    "    while True:\n",
    "        count += 1\n",
    "        # print('\\n','\\n','\\n','\\n','\\n')\n",
    "        \n",
    "        # \n",
    "        #adding a cost for switching the states\n",
    "        if count%counter == 0:\n",
    "            print(count)\n",
    "        # print(count)\n",
    "\n",
    "        rewardfu = utility_calculation(pomdp, future_step)\n",
    "        \n",
    "        # print('Rewardddddd. ', pomdp.reward_arr)\n",
    "        # print('rewardfu: ',rewardfu)\n",
    "        \n",
    "        # print('HEEEEEEEEEEEREEEEEEEEEE')\n",
    "        # print('\\n')\n",
    "        # print('rewardfu: ', rewardfu)\n",
    "        # print('EEE: ', pomdp.e_prob)\n",
    "        \n",
    "        cost_added = np.zeros((len(pomdp.actions), len(pomdp.states)), dtype=float)\n",
    "        # print('costadded: ',cost_added)\n",
    "        for i in range(len(rewardfu)):\n",
    "            # print('1: ', rewardfu[i][0])\n",
    "            # print('1*e: ',np.sum(pomdp.e_prob * rewardfu[i][0]))\n",
    "            max =np.sum(pomdp.belief * rewardfu[i][0])\n",
    "            arg = 0\n",
    "            for j in range(1, len(rewardfu[i])):\n",
    "                # print(f'{j}: ', rewardfu[i][j])\n",
    "                # print(f'{j}*e: ',np.sum(pomdp.e_prob * rewardfu[i][j]))\n",
    "                now = np.sum(pomdp.belief * rewardfu[i][j])\n",
    "                if now > max:\n",
    "                    # print('Now')\n",
    "                    max = now\n",
    "                    arg = j\n",
    "            cost_added[i] = rewardfu[i][arg]\n",
    "        # if pomdp.statelist[-1] == 0:\n",
    "        #     cost_added = rewardfu  \n",
    "        # else: \n",
    "        #     cost_added = rewardfu \n",
    "            \n",
    "        # print('costadded: ',cost_added)\n",
    "        # print('\\n')\n",
    "        # print(count)\n",
    "        # print('1. ', pomdp.reward_arr)\n",
    "        # print('2. ' ,pomdp.fu_reward_arr)\n",
    "        # print('3. ', pomdp.max_fu_reward)\n",
    "        # print('cost added. ', cost_added)   \n",
    "        # cost_added = pomdp.reward_arr + np.array([[0,switching_cost],[0,switching_cost]])\n",
    "        # hypo_belief = pomdp.belief_generation()\n",
    "        # possible_outcome = rewardfu * hypo_belief\n",
    "        \n",
    "        \n",
    "        # print('possible outcome: ', future_outcome)\n",
    "        # if pomdp.statelist[-1] == 0:\n",
    "        #     # cost_added = future_outcome + pomdp.reward_arr +  np.array([[switching_cost,0],[switching_cost,0]])\n",
    "        #     cost_added = future_outcome + pomdp.reward_arr\n",
    "        # else: \n",
    "        #     # cost_added = future_outcome + pomdp.reward_arr +  np.array([[0,switching_cost],[0,switching_cost]])\n",
    "        #     cost_added = future_outcome + pomdp.reward_arr\n",
    "        \n",
    "        # print('cost added: ', cost_added)\n",
    "        # cost_added = rewardfu\n",
    "        # print('belief ',pomdp.belief)\n",
    "        U = pomdp.belief * cost_added\n",
    "        # print('U: ', U)\n",
    "        \n",
    "        # print(possible_outcome)\n",
    "        \n",
    "\n",
    "        \n",
    "        # print('e ',pomdp.belief)\n",
    "        # print('r ', pomdp.reward_arr)\n",
    "        # print('U ',U)\n",
    "        \n",
    "\n",
    "        # determines the more valued action for both states (for example, if the agent was in the state 0, which action would be more valued. and vice versa)\n",
    "        # print(action)\n",
    "        \n",
    "        outcome = pomdp.outcome(U)\n",
    "        # print('o: ', outcome)\n",
    "        \n",
    "        if prev == '0':\n",
    "            outcome = outcome - np.array([0,switching_cost, switching_cost, 0, 0])\n",
    "        elif prev == '1':\n",
    "            outcome = outcome - np.array([switching_cost,0, 0, 0, 0])\n",
    "        else: \n",
    "            outcome = outcome - np.array([switching_cost,0 ,0 , 0, 0])\n",
    "        # print('outcome: ', outcome)\n",
    "        # now we select action based on the previous state,\n",
    "        act = pomdp.selected_action(outcome)\n",
    "        # print('action: ', act)\n",
    "        state = pomdp.last_state(act)\n",
    "        #storing times which agent spends between two switches\n",
    "        # print('state: ', state,'\\n')\n",
    "\n",
    "        \n",
    "        if prev == '0':\n",
    "            if state == '0':\n",
    "                c0 += 1\n",
    "                c1 = 1\n",
    "\n",
    "            elif state == '1' or state == '2':\n",
    "                duration[0].append(c0)\n",
    "                all_states_duration[0].append(c0)\n",
    "\n",
    "        elif prev == '1' or prev == '2':\n",
    "            if state == '1' or state == '2':\n",
    "                c1 += 1\n",
    "                c0 = 1\n",
    "            elif state == '0':\n",
    "                duration[1].append(c1)\n",
    "\n",
    "\n",
    "\n",
    "        prev = state\n",
    "\n",
    "        # sacing values in each time point\n",
    "        ulist[0].append(outcome[0])\n",
    "        ulist[1].append(outcome[1])\n",
    "        ulist[2].append(outcome[2])\n",
    "        # print(act)\n",
    "\n",
    "        # choosing the next state baseed on the action and transition probabilities\n",
    "        # print('\\n','\\n')\n",
    "\n",
    "        # updates for the next time point \n",
    "\n",
    "        # print('count', count)\n",
    "        # print('Rewardddddd. ', pomdp.reward_arr)\n",
    "        # print('rewardfu: ',rewardfu)\n",
    "        # print('belief ',pomdp.belief)\n",
    "        # print('U: ', U)\n",
    "        # print('o: ', outcome)\n",
    "        # print('action: ', act)\n",
    "        # print('state: ', state,'\\n')\n",
    "        \n",
    "        pomdp.update(state,act)\n",
    "        pomdp.belief_update()\n",
    "        pomdp.reward_set()\n",
    "\n",
    "        if count > max_count:\n",
    "            # belief0 = np.array(pomdp.belieflist)\n",
    "            # belief1 = 1-belief0\n",
    "            # belief = np.array((belief0,belief1))\n",
    "            # state = np.array(pomdp.statelist)\n",
    "            # reward = np.array(pomdp.rewardlist)\n",
    "            \n",
    "            # plt.plot(reward[0],color = 'r',label='reward left')\n",
    "            # plt.plot(reward[1],color = 'b',label='reward right')\n",
    "            # plt.title(f'Reward Plot')\n",
    "            # plt.legend()\n",
    "            # plt.show()  \n",
    "\n",
    "            # plt.plot(ulist[0],color = 'g',label='value left')\n",
    "            # plt.plot(ulist[1],color = 'purple',label='value right')\n",
    "            # plt.title(f'Reward Plot')\n",
    "            # plt.legend()\n",
    "            # plt.show()\n",
    "\n",
    "\n",
    "            # plot_fish(state)\n",
    "            # plot_belief(belief)\n",
    "\n",
    "            # return duration\n",
    "                \n",
    "            # for i in range(1):\n",
    "                # # print(ulist[0])\n",
    "                # interval = 200\n",
    "                # start_point = random.randint(1, count - interval)\n",
    "                # end_point = start_point + interval\n",
    "                # # belief0 = np.array(pomdp.belieflist)[start_point:end_point]\n",
    "                # # belief1 = 1-belief0\n",
    "                # # belief = np.array((belief0,belief1))\n",
    "                # state = np.array(pomdp.statelist)\n",
    "                # # reward = np.array(pomdp.rewardlist)\n",
    "                \n",
    "                # # plt.plot(reward[0][start_point:end_point],color = 'r',label='reward left')\n",
    "                # # plt.plot(reward[1][start_point:end_point],color = 'b',label='reward right')\n",
    "                # # plt.title(f'Reward Plot from {start_point} to {end_point}')\n",
    "                # # plt.legend()\n",
    "                # # plt.show()  \n",
    "\n",
    "                # plt.plot(ulist[0][start_point:end_point],color = 'g',label='value Pinwheel')\n",
    "                # plt.plot(ulist[1][start_point:end_point],color = 'purple',label=\"value H Bull's-eye\")\n",
    "                # plt.plot(ulist[2][start_point:end_point],color = 'r',label=\"value V Bull's-eye\")\n",
    "                # plt.title(f'Reward Plot from {start_point} to {end_point}')\n",
    "                # plt.legend()\n",
    "                # plt.show()\n",
    "\n",
    "\n",
    "                # # plot_fish(state[start_point:end_point])\n",
    "                # # plot_belief(belief)\n",
    "\n",
    "                # print('\\n','\\n','\\n')\n",
    "            \n",
    "            return duration\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1):\n",
    "#     time = test_pomdp_value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tim = [[],[]]\n",
    "# time = test_pomdp_value_iteration()\n",
    "# tim[0].extend(time[0])\n",
    "# tim[1].extend(time[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rivalry_dynamics(c1, c2, key_rew, punish1, punish2, obs_noise, p_stay, trnasition_proportion, gamma, future_steps, switching_cost, bias):\n",
    "    # rewards = (rewDom, c1, c2, alpha , expts, key_rew, punish_rew_1, punish_rew_2, consfixed)\n",
    "    rewards = (10, c1, c2, 0.5, 5, key_rew, punish1, punish2, 0, bias[0], bias[1])\n",
    "    # def __init__(self, actions, transitions=None, evidences=None, rewards=None, states=None, gamma=0.95)\n",
    "    sigmaAttention = obs_noise\n",
    "    p_stay = p_stay\n",
    "    gamma = gamma\n",
    "    a,b = trnasition_proportion\n",
    "    c = a+b\n",
    "    transitions = np.array([[p_stay, (1-p_stay)/2, (1-p_stay)/2],\n",
    "                                [b*(1-p_stay)/c, p_stay, a*(1-p_stay)/c],\n",
    "                                [b*(1-p_stay)/c, a*(1-p_stay)/c, p_stay]])\n",
    "    # print('transitions', transitions)\n",
    "    actions = ('0', '1', '2', '3', '4')\n",
    "    states = ('0', '1', '2')\n",
    "    Initial_loc = 0\n",
    "\n",
    "\n",
    "    future_steps = future_steps\n",
    "    iterations = 5000\n",
    "    counter = 20000\n",
    "    switching_cost = switching_cost\n",
    "    time  = test_pomdp_value_iteration(actions, rewards, transitions, states, p_stay, sigmaAttention, gamma, future_steps, iterations, switching_cost, Initial_loc,counter)\n",
    "\n",
    "\n",
    "\n",
    "    # side = {0: 'Pinwheel', 1: 'Bulls-eye'}\n",
    "\n",
    "    # fig, axs = plt.subplots(1, 2, figsize=(12, 6), sharey=True)\n",
    "\n",
    "    # for j in range(2):\n",
    "    #     data = time[j]\n",
    "\n",
    "    #     axs[j].hist(data, bins=250, alpha=0.7, label='Histogram')\n",
    "\n",
    "    #     mean = np.mean(data)\n",
    "    #     std = np.std(data)\n",
    "\n",
    "    #     # Add mean and standard deviation to the legend\n",
    "    #     axs[j].legend([f\"Mean: {mean:.2f}, SD: {std:.2f}\"], loc='upper right')\n",
    "\n",
    "    #     axs[j].set_xlabel(\"Time Duration\")\n",
    "    #     axs[j].set_ylabel(\"Frequency\" if j == 0 else \"\")\n",
    "    #     axs[j].set_title(f'{side[j]} Histogram')\n",
    "\n",
    "        # ks_statistic, p_value = stats.kstest(data, 'gamma', args=(2,))\n",
    "        # print(f\"KS Test: {ks_statistic}, p-value: {p_value}\")\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    total_time = [sum(state_time) for state_time in time]\n",
    "\n",
    "    total_sum = sum(total_time)\n",
    "    proportions = [t / total_sum for t in total_time]\n",
    "\n",
    "    # states = ['Pin Wheel', \"Bull's Eye\"]\n",
    "\n",
    "    # plt.figure(figsize=(10, 6))\n",
    "    # bars = plt.bar(states, proportions, color=['red', 'blue'])\n",
    "\n",
    "    # for bar in bars:\n",
    "    #     height = bar.get_height()\n",
    "    #     plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "    #             f'{height:.2%}',\n",
    "    #             ha='center', va='bottom')\n",
    "\n",
    "    # plt.title('Proportion of Time Spent in Each State')\n",
    "    # plt.ylabel('Proportion of Total Time')\n",
    "    # plt.ylim(0, 1) \n",
    "\n",
    "    # plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # plt.savefig('state_proportions.png')\n",
    "    # plt.show()\n",
    "\n",
    "    return proportions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0.6748356246264196\n",
      "1\n",
      "0.6979466119096509\n"
     ]
    }
   ],
   "source": [
    "pred = [[],[]]\n",
    "for i in range(2):\n",
    "    print(i)\n",
    "    tr = 0.5*(i/12)+1\n",
    "    o1 = 0.1-(i/11)*0.02\n",
    "    o2 = 0.1-(i/11)*0.05\n",
    "    c1, c2, key_rew, punish1, punish2, obs_noise, p_stay, trnasition_proportion, gamma, future_steps, switching_cost, bias = -3, -3, 3, 4, 4, np.array([0.02, o1, o2]), 0.6, [tr,1], 0, 0, 3, [1,1]\n",
    "    output1 = rivalry_dynamics(c1, c2, key_rew, punish1, punish2, obs_noise, p_stay, trnasition_proportion, gamma, future_steps, switching_cost, bias)\n",
    "    pred[0].append(output1[0])\n",
    "    print(output1[1])\n",
    "    pred[1].append(output1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Data\n",
    "# x = np.log(x)\n",
    "x = np.array([i for i in range(0,2)])\n",
    "y1 = (np.array(pred[0]))\n",
    "y2 = (np.array(pred[1]))\n",
    "\n",
    "plt.plot(x, y1, label='y1')  # first line\n",
    "plt.plot(x, y2, label='y2')  # second line\n",
    "# Plot\n",
    "# plt.plot(x, log_y)  # line with markers at each point\n",
    "plt.title(\"Trend of y over x\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# plt.savefig(\"Plots/plot.png\")\n",
    "# plt.close()\n",
    "# from IPython.display import Image\n",
    "# Image(\"Plots/plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
